{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5sGkk5lvio",
        "outputId": "b9140750-e7ae-483a-8576-063a8493ec56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pip==23.0.1\n",
            "  Downloading pip-23.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-23.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pip==23.0.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcxyTC_dl480",
        "outputId": "21a35d24-4145-4b34-b8bb-3f1ec0f08861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.16.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288619 sha256=b2fada15c944ca2f9bed0aec443f5aa362d06390deace6562243266245ebe489\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=fc946e95239eb6ec810b489a14d485f179de6fafbc32628c5a127022b9a24c2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu fairseq sentencepiece wandb gdown pandas torch numpy matplotlib tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Am8SiuA4l-vH",
        "outputId": "db67aefa-d171-46bd-f2d6-f7874f3a75f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Sys5keuiw4C27_cG3LMyYi6v5eHJvu1L\n",
            "From (redirected): https://drive.google.com/uc?id=1Sys5keuiw4C27_cG3LMyYi6v5eHJvu1L&confirm=t&uuid=7b34948a-854d-4d76-b03e-5830d3330a7a\n",
            "To: /content/DATA.zip\n",
            "100% 28.2M/28.2M [00:00<00:00, 33.9MB/s]\n",
            "Archive:  DATA.zip\n",
            "   creating: DATA/\n",
            "  inflating: DATA/.DS_Store          \n",
            "  inflating: __MACOSX/DATA/._.DS_Store  \n",
            "   creating: DATA/rawdata/\n",
            "   creating: DATA/rawdata/ted2020/\n",
            "  inflating: DATA/rawdata/.DS_Store  \n",
            "  inflating: __MACOSX/DATA/rawdata/._.DS_Store  \n",
            "  inflating: DATA/rawdata/ted2020/test.raw.zh  \n",
            "  inflating: __MACOSX/DATA/rawdata/ted2020/._test.raw.zh  \n",
            "  inflating: DATA/rawdata/ted2020/test.raw.en  \n",
            "  inflating: __MACOSX/DATA/rawdata/ted2020/._test.raw.en  \n",
            "  inflating: DATA/rawdata/ted2020/train_dev.raw.zh  \n",
            "  inflating: __MACOSX/DATA/rawdata/ted2020/._train_dev.raw.zh  \n",
            "  inflating: DATA/rawdata/ted2020/train_dev.raw.en  \n",
            "  inflating: __MACOSX/DATA/rawdata/ted2020/._train_dev.raw.en  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1Sys5keuiw4C27_cG3LMyYi6v5eHJvu1L' --output DATA.zip\n",
        "!unzip -o DATA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pprint\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "from fairseq.data import iterators\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import sacrebleu\n",
        "import pandas as pd\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "from fairseq.models import FairseqEncoderDecoderModel\n",
        "from fairseq.models.transformer import TransformerEncoder, TransformerDecoder\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "config = Namespace(\n",
        "    src_lang='en',\n",
        "    tgt_lang='zh',\n",
        "\n",
        "    data_dir='./DATA/rawdata',\n",
        "    dataset_name='ted2020',\n",
        "    prefix=Path('./DATA/rawdata').absolute() / 'ted2020',\n",
        "\n",
        "    train_prefix='./DATA/rawdata/ted2020/train_dev.raw',\n",
        "    test_prefix='./DATA/rawdata/ted2020/test.raw',\n",
        "\n",
        "    vocab_size=8000,\n",
        "    max_len=1024,\n",
        "\n",
        "    spm_model_path='./DATA/sentencepiece.model',\n",
        "\n",
        "    data_bin_path='./DATA/data-bin/ted2020',\n",
        "\n",
        "    savedir='./checkpoints/transformer',\n",
        "    num_workers=4,\n",
        "    max_tokens=2048,\n",
        "    accum_steps=4,\n",
        "    lr_factor=1.0,\n",
        "    lr_warmup=4000,\n",
        "    clip_norm=1.0,\n",
        "    max_epoch=5,\n",
        "    start_epoch=1,\n",
        "    beam=5,\n",
        "    max_len_a=1.2,\n",
        "    max_len_b=10,\n",
        "\n",
        "    post_process=\"sentencepiece\",\n",
        "\n",
        "    keep_last_epochs=5,\n",
        "    resume=None,\n",
        "\n",
        "    early_stopping=True,\n",
        "    early_stopping_patience=7,\n",
        ")\n"
      ],
      "metadata": {
        "id": "UnYI_2Dn38CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = Path(config.prefix)\n",
        "train_dev_en = prefix / 'train_dev.raw.en'\n",
        "train_dev_zh = prefix / 'train_dev.raw.zh'\n",
        "test_en = prefix / 'test.raw.en'\n",
        "test_zh = prefix / 'test.raw.zh'\n",
        "\n",
        "tokenized_data_dir = Path('./DATA/tokenized')\n",
        "tokenized_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "spm_model_path = Path(config.spm_model_path)\n",
        "\n",
        "if not spm_model_path.exists():\n",
        "    import sentencepiece as spm\n",
        "    combined_train_valid = tokenized_data_dir / 'combined_train_valid.txt'\n",
        "    with open(combined_train_valid, 'w', encoding='utf-8') as outfile, \\\n",
        "         open(train_dev_en, 'r', encoding='utf-8') as en_f, \\\n",
        "         open(train_dev_zh, 'r', encoding='utf-8') as zh_f:\n",
        "        for en_line, zh_line in zip(en_f, zh_f):\n",
        "            if en_line.strip() and zh_line.strip():\n",
        "                outfile.write(en_line.strip() + '\\n')\n",
        "                outfile.write(zh_line.strip() + '\\n')\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=str(combined_train_valid),\n",
        "        model_prefix=str(spm_model_path.with_suffix('')),\n",
        "        vocab_size=config.vocab_size,\n",
        "        model_type='bpe',\n",
        "        character_coverage=1.0,\n",
        "        unk_id=0,\n",
        "        pad_id=1,\n",
        "        bos_id=2,\n",
        "        eos_id=3,\n",
        "    )\n",
        "    print(\"SentencePiece model trained.\")\n",
        "else:\n",
        "    print(\"SentencePiece model already exists. Skipping training.\")\n",
        "\n",
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model.load(str(spm_model_path))\n",
        "\n",
        "def tokenize_and_truncate(input_file, output_file, spm_model, max_len):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                tokens = spm_model.encode(line, out_type=str)\n",
        "                if len(tokens) > max_len:\n",
        "                    tokens = tokens[:max_len]\n",
        "                outfile.write(' '.join(tokens) + '\\n')\n",
        "\n",
        "train_en_tok = tokenized_data_dir / 'train.en'\n",
        "train_zh_tok = tokenized_data_dir / 'train.zh'\n",
        "if not train_en_tok.exists() or not train_zh_tok.exists():\n",
        "    print(\"Tokenizing training data...\")\n",
        "    tokenize_and_truncate(train_dev_en, train_en_tok, spm_model, config.max_len)\n",
        "    tokenize_and_truncate(train_dev_zh, train_zh_tok, spm_model, config.max_len)\n",
        "    print(\"Training data tokenization completed.\")\n",
        "else:\n",
        "    print(\"Tokenized training data already exists. Skipping tokenization.\")\n",
        "\n",
        "test_en_tok = tokenized_data_dir / 'test.en'\n",
        "test_zh_tok = tokenized_data_dir / 'test.zh'\n",
        "if not test_en_tok.exists() or not test_zh_tok.exists():\n",
        "    print(\"Tokenizing test data...\")\n",
        "    tokenize_and_truncate(test_en, test_en_tok, spm_model, config.max_len)\n",
        "    tokenize_and_truncate(test_zh, test_zh_tok, spm_model, config.max_len)\n",
        "    print(\"Test data tokenization completed.\")\n",
        "else:\n",
        "    print(\"Tokenized test data already exists. Skipping tokenization.\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "valid_size = 0.05\n",
        "\n",
        "train_en_final = tokenized_data_dir / 'train.final.en'\n",
        "train_zh_final = tokenized_data_dir / 'train.final.zh'\n",
        "valid_en_final = tokenized_data_dir / 'valid.en'\n",
        "valid_zh_final = tokenized_data_dir / 'valid.zh'\n",
        "\n",
        "if not train_en_final.exists() or not valid_en_final.exists():\n",
        "    print(\"Splitting training data into train and validation sets...\")\n",
        "    with open(train_en_tok, 'r', encoding='utf-8') as en_f, \\\n",
        "         open(train_zh_tok, 'r', encoding='utf-8') as zh_f:\n",
        "        en_lines = [line.strip() for line in en_f if line.strip()]\n",
        "        zh_lines = [line.strip() for line in zh_f if line.strip()]\n",
        "\n",
        "    en_train, en_valid, zh_train, zh_valid = train_test_split(\n",
        "        en_lines, zh_lines, test_size=valid_size, random_state=seed\n",
        "    )\n",
        "\n",
        "    with open(train_en_final, 'w', encoding='utf-8') as en_train_f, \\\n",
        "         open(train_zh_final, 'w', encoding='utf-8') as zh_train_f:\n",
        "        for en, zh in zip(en_train, zh_train):\n",
        "            en_train_f.write(en + '\\n')\n",
        "            zh_train_f.write(zh + '\\n')\n",
        "\n",
        "    with open(valid_en_final, 'w', encoding='utf-8') as en_valid_f, \\\n",
        "         open(valid_zh_final, 'w', encoding='utf-8') as zh_valid_f:\n",
        "        for en, zh in zip(en_valid, zh_valid):\n",
        "            en_valid_f.write(en + '\\n')\n",
        "            zh_valid_f.write(zh + '\\n')\n",
        "    print(\"Data splitting completed.\")\n",
        "else:\n",
        "    print(\"Train and validation sets already exist. Skipping data splitting.\")\n"
      ],
      "metadata": {
        "id": "7bsb-pgN5NOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not Path(config.data_bin_path).exists():\n",
        "    print(\"Preprocessing data with Fairseq...\")\n",
        "    os.makedirs(config.data_bin_path, exist_ok=True)\n",
        "    os.system(\n",
        "        f\"fairseq-preprocess \"\n",
        "        f\"--source-lang en \"\n",
        "        f\"--target-lang zh \"\n",
        "        f\"--trainpref {tokenized_data_dir}/train.final \"\n",
        "        f\"--validpref {tokenized_data_dir}/valid \"\n",
        "        f\"--testpref {tokenized_data_dir}/test \"\n",
        "        f\"--destdir {config.data_bin_path} \"\n",
        "        f\"--joined-dictionary \"\n",
        "        f\"--workers {config.num_workers}\"\n",
        "    )\n",
        "    print(\"Fairseq preprocessing completed.\")\n",
        "else:\n",
        "    print(\"Fairseq binary data already exists. Skipping preprocessing.\")\n"
      ],
      "metadata": {
        "id": "CT_p4-Gw5NWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.data_bin_path,\n",
        "    source_lang=config.src_lang,\n",
        "    target_lang=config.tgt_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)\n",
        "print(\"Loading datasets...\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True)\n",
        "task.load_dataset(split=\"valid\", epoch=1)\n",
        "task.load_dataset(split=\"test\", epoch=1)\n"
      ],
      "metadata": {
        "id": "qMmKIuSB5Ndo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(args, task):\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    model = FairseqEncoderDecoderModel(encoder, decoder)\n",
        "\n",
        "    def init_params(module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
        "        elif isinstance(module, nn.MultiheadAttention):\n",
        "            nn.init.xavier_uniform_(module.out_proj.weight)\n",
        "            if module.out_proj.bias is not None:\n",
        "                nn.init.constant_(module.out_proj.bias, 0)\n",
        "\n",
        "    model.apply(init_params)\n",
        "    return model\n",
        "\n",
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=256,\n",
        "    encoder_ffn_embed_dim=1024,\n",
        "    encoder_layers=3,\n",
        "    encoder_attention_heads=4,\n",
        "    encoder_normalize_before=True,\n",
        "    decoder_embed_dim=256,\n",
        "    decoder_ffn_embed_dim=1024,\n",
        "    decoder_layers=3,\n",
        "    decoder_attention_heads=4,\n",
        "    decoder_normalize_before=True,\n",
        "    share_decoder_input_output_embed=False,\n",
        "    dropout=0.1,\n",
        "    activation_fn=\"relu\",\n",
        "    max_source_positions=1024,\n",
        "    max_target_positions=1024,\n",
        ")\n",
        "\n",
        "model = build_model(arch_args, task)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "t4ReWmyh5NjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    if step_num == 0:\n",
        "        step_num = 1\n",
        "    return (d_model ** -0.5) * min(step_num ** -0.5, step_num * (warmup_step ** -1.5))\n",
        "\n",
        "class NoamOpt:\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def multiply_grads(self, c):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * get_rate(self.model_size, step, self.warmup)\n",
        "\n",
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim,\n",
        "    factor=config.lr_factor,\n",
        "    warmup=config.lr_warmup,\n",
        "    optimizer=torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0,\n",
        "        betas=(0.9, 0.98),\n",
        "        eps=1e-9,\n",
        "        weight_decay=0.0001\n",
        "    )\n",
        ")\n",
        "\n",
        "steps = np.arange(1, 10001)\n",
        "lrs = [optimizer.rate(i) for i in steps]\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(steps, lrs)\n",
        "plt.title(\"Noam Learning Rate Schedule\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QGeDDxDy5Nm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(toks, dictionary):\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    sequence_generator = task.build_generator([model], config)\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()),\n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"],\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()),\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs\n",
        "\n",
        "def validate(model, task, criterion, log_to_console=True):\n",
        "    print('Beginning validation...')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "\n",
        "    stats = {\"loss\": [], \"bleu\": 0, \"srcs\": [], \"hyps\": [], \"refs\": []}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "\n",
        "    tok = 'zh'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok)\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "\n",
        "    if len(hyps) > 0 and log_to_console:\n",
        "        showid = np.random.randint(len(hyps))\n",
        "        print(\"Example Source: \" + srcs[showid])\n",
        "        print(\"Example Hypothesis: \" + hyps[showid])\n",
        "        print(\"Example Reference: \" + refs[showid])\n",
        "\n",
        "    if log_to_console:\n",
        "        print(f\"Validation Loss: {stats['loss']:.4f}\")\n",
        "        print(f\"Validation BLEU: {stats['bleu'].score:.2f}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):\n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir / f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir / f\"checkpoint{epoch}.pt\", savedir / f\"checkpoint_last.pt\")\n",
        "        print(f\"Saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "\n",
        "        with open(savedir / f\"samples{epoch}.en-zh.txt\", \"w\", encoding='utf-8') as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir / f\"checkpoint_best.pt\")\n",
        "            print(f\"New best checkpoint saved: {savedir}/checkpoint_best.pt\")\n",
        "\n",
        "        del_epoch = epoch - config.keep_last_epochs\n",
        "        if del_epoch >= config.start_epoch:\n",
        "            del_file = savedir / f\"checkpoint{del_epoch}.pt\"\n",
        "            if del_file.exists():\n",
        "                del_file.unlink()\n",
        "                print(f\"Deleted old checkpoint: {del_file}\")\n",
        "\n",
        "    if config.early_stopping:\n",
        "        if bleu.score > getattr(validate_and_save, \"best_bleu\", 0):\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            validate_and_save.patience = config.early_stopping_patience\n",
        "        else:\n",
        "            validate_and_save.patience -= 1\n",
        "            print(f\"Early stopping patience remaining: {validate_and_save.patience}\")\n",
        "            if validate_and_save.patience <= 0:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                return False\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "_ycLM-xj5Ntj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device=device)\n",
        "\n",
        "print(\"Task: {}\".format(task.__class__.__name__))\n",
        "print(\"Encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "print(\"Decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "print(\"Criterion: {}\".format(criterion.__class__.__name__))\n",
        "print(\"Optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "print(\n",
        "    \"Number of model parameters: {:,} (trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "print(f\"Max tokens per batch = {config.max_tokens}, Accumulate steps = {config.accum_steps}\")\n",
        "\n",
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=0, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir) / name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath, map_location=device)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer is not None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        print(f\"Loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        print(f\"No checkpoints found at {checkpath}!\")\n",
        "\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps)\n",
        "\n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"Train Epoch {epoch_itr.epoch}\", leave=True)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)\n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "            accum_loss += loss.item()\n",
        "\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_print = accum_loss / sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        print(f\"Train Epoch {epoch_itr.epoch} | Loss: {loss_print:.4f}\")\n",
        "\n",
        "    loss_mean = np.mean(stats[\"loss\"])\n",
        "    print(f\"Training Loss for Epoch {epoch_itr.epoch}: {loss_mean:.4f}\")\n",
        "    return stats\n",
        "\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    continue_training = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    print(f\"End of Epoch {epoch_itr.epoch}\")\n",
        "    if config.early_stopping and not continue_training:\n",
        "        break\n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)\n",
        "\n",
        "checkdir = Path(config.savedir)\n",
        "\n",
        "avg_checkpoint_path = checkdir / \"avg_last_5_checkpoint.pt\"\n",
        "if not avg_checkpoint_path.exists():\n",
        "    checkpoint_files = sorted(checkdir.glob(\"checkpoint*.pt\"), key=lambda x: x.stem)\n",
        "    last_five = checkpoint_files[-5:]\n",
        "    if len(last_five) >= 1:\n",
        "        checkpoints_str = ' '.join([str(cp) for cp in last_five])\n",
        "        os.system(f\"fairseq-average-checkpoints --inputs {checkpoints_str} --num-epoch-checkpoints {len(last_five)} --output {avg_checkpoint_path}\")\n",
        "        print(f\"Averaged last {len(last_five)} checkpoints into {avg_checkpoint_path}\")\n",
        "    else:\n",
        "        print(\"Not enough checkpoints to average.\")\n",
        "else:\n",
        "    print(f\"Averaged checkpoint already exists at {avg_checkpoint_path}\")\n",
        "\n",
        "best_checkpoint = checkdir / \"checkpoint_best.pt\"\n",
        "if best_checkpoint.exists():\n",
        "    check = torch.load(best_checkpoint, map_location=device)\n",
        "    model.load_state_dict(check[\"model\"])\n",
        "    print(\"Loaded best checkpoint for final evaluation.\")\n",
        "    validate(model, task, criterion, log_to_console=True)\n",
        "else:\n",
        "    print(\"Best checkpoint not found. Skipping loading best checkpoint.\")\n",
        "\n",
        "task.load_dataset(split=\"test\", epoch=1)\n",
        "test_itr = load_data_iterator(task, \"test\", epoch=1, max_tokens=config.max_tokens, num_workers=0).next_epoch_itr(shuffle=False)\n",
        "\n",
        "idxs = []\n",
        "hyps = []\n",
        "\n",
        "model.eval()\n",
        "progress = tqdm.tqdm(test_itr, desc=f\"Prediction\", leave=True)\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(progress):\n",
        "        sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "        s, h, r = inference_step(sample, model)\n",
        "\n",
        "        hyps.extend(h)\n",
        "        idxs.extend(list(sample['id']))\n",
        "\n",
        "sorted_hyps = [x for _, x in sorted(zip(idxs, hyps))]\n",
        "\n",
        "pred = pd.DataFrame({\n",
        "    'id': range(len(sorted_hyps)),\n",
        "    'sentence': sorted_hyps\n",
        "})\n",
        "\n",
        "pred.to_csv('prediction.csv', index=False)\n",
        "print(\"Saved predictions to prediction.csv\")\n",
        "\n",
        "combined_train_valid = tokenized_data_dir / 'combined_train_valid.txt'\n",
        "if combined_train_valid.exists():\n",
        "    combined_train_valid.unlink()\n",
        "    print(f\"Deleted {combined_train_valid}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W6Djhxhl38Fh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}